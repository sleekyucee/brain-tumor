# **Real-Time MRI Brain-Tumor Classification**


<font size="4">The field of medical diagnostics has witnessed a transformation in recent years, with the advent of deep learning and artificial intelligence technologies. Leveraging these innovations, this notebook presents a comprehensive analysis of a real-time MRI brain tumor classification system. The system's objective is to swiftly and accurately classify MRI brain scans into four categories: normal, glioma, meningioma, and pituitary tumors. This analysis demonstrates the capabilities of various deep learning models and the importance of a robust dataset in training models for effective tumor classification.

**Dataset Information:**

In the search for suitable data sources, the Kaggle platform yielded a dataset initially shared by Thomas Dubail. This publicly available dataset consists of 3,096 image files organized into four directories: "glioma_tumor" (901 image files), "meningioma_tumor" (913 image files), "normal" (438 image files), and "pituitary_tumor" (844 image files). This dataset played a pivotal role as the foundation for this research, offering a wide range of brain tumor images for thorough analysis. The dataset's high quality and diversity are essential for training deep learning models to achieve accurate tumor classification.

**Background on Brain Tumors:**

Brain tumors, medically known as intracranial tumors, encompass a wide range of masses or abnormal cell growths within the brain or its immediate vicinity. These enigmatic intruders manifest in over 120 distinct forms, including chordomas, gangliocytomas, glomus jugulare, gliomas, meningiomas, pituitary adenomas, and craniopharyngiomas, to name a few.

While some brain tumors, such as benign ones, exhibit gradual growth, others, categorized as malignant tumors, proliferate rapidly and pose significant health challenges. This study focuses on three of the most prevalent brain tumor types: glioma, meningioma, and pituitary tumors, while also considering a fourth category denoting the absence of such abnormalities - "normal."

The importance of early detection and accurate classification of brain tumors cannot be overstated, as it plays a pivotal role in patient prognosis and treatment planning. By harnessing the capabilities of deep learning, this research aims to develop a real-time MRI brain tumor classification system that enhances diagnostic speed, accuracy, and overall healthcare efficiency. The subsequent sections provide a detailed analysis of the models and recommendations for future advancements in this critical field of medical diagnostics.</font>
# **Import Necessary Dependencies and Libraries**
!pip install opendatasets
!pip install torchsampler
# PyTorch libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import Adam, SGD
import torch.nn.functional as F
import torchvision.models as models
from torchvision.models import ResNet50_Weights
from torchvision.models import VGG11_BN_Weights

# Data handling libraries
import pandas as pd
import numpy as np
import torchvision.transforms as transforms
from torchvision import datasets
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, SubsetRandomSampler
from tqdm import tqdm
import copy

# Scikit-learn libraries
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, StratifiedKFold, ParameterGrid

# System and utilities
import sys
sys.path.append("path")
import warnings
warnings.filterwarnings("ignore")
import pickle

# File and folder manipulation
import os
import cv2
import shutil
import random
from IPython.display import FileLink

# Data visualization
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from IPython.display import display
from PIL import Image
import seaborn as sns
import plotly.express as px

# Time tracking
import time

# Custom libraries
from torchsampler import ImbalancedDatasetSampler
# **Data Importing & Exploration**
od.download('https://www.kaggle.com/datasets/thomasdubail/brain-tumors-256x256')
# Define the data directory and subdirectories
data_dir = '/content/brain-tumors-256x256/Data'
glioma_dir = os.path.join(data_dir, 'glioma_tumor')
meningioma_dir = os.path.join(data_dir, 'meningioma_tumor')
normal_dir = os.path.join(data_dir, 'normal')
pituitary_dir = os.path.join(data_dir, 'pituitary_tumor')

# Define training, validation, and test directory paths
train_dir = os.path.join(data_dir, 'train')
test_dir = os.path.join(data_dir, 'test')
val_dir = os.path.join(data_dir, 'val')


# Create subdirectories for each tumor type within train_dir, test_dir, and val_dir
for tumor_type in ['glioma_tumor', 'meningioma_tumor', 'normal', 'pituitary_tumor']:
    os.makedirs(os.path.join(train_dir, tumor_type), exist_ok=True)
    os.makedirs(os.path.join(test_dir, tumor_type), exist_ok=True)
    os.makedirs(os.path.join(val_dir, tumor_type), exist_ok=True)
# Get the number of samples for each class
num_glioma_samples = len(os.listdir(glioma_dir))
num_meningioma_samples = len(os.listdir(meningioma_dir))
num_normal_samples = len(os.listdir(normal_dir))
num_pituitary_samples = len(os.listdir(pituitary_dir))

# Get the total number of samples in the entire dataset
total_samples = num_glioma_samples + num_meningioma_samples + num_normal_samples + num_pituitary_samples

# Display results
print(f"Number of glioma tumor samples: {num_glioma_samples}")
print(f"Number of meningioma tumor samples: {num_meningioma_samples}")
print(f"Number of normal samples: {num_normal_samples}")
print(f"Number of pituitary tumor samples: {num_pituitary_samples}")
print(f"Total number of samples in the dataset: {total_samples}")
# Calculate percentage count for each class
percentage_glioma = (num_glioma_samples / total_samples) * 100
percentage_meningioma = (num_meningioma_samples / total_samples) * 100
percentage_normal = (num_normal_samples / total_samples) * 100
percentage_pituitary = (num_pituitary_samples / total_samples) * 100

# Display results
print(f"Percentage of glioma tumor samples: {percentage_glioma:.2f}%")
print(f"Percentage of meningioma tumor samples: {percentage_meningioma:.2f}%")
print(f"Percentage of normal samples: {percentage_normal:.2f}%")
print(f"Percentage of pituitary tumor samples: {percentage_pituitary:.2f}%")
print(f"Total number of samples in the dataset: {total_samples}")

# Bar plot to visualize the number of samples
classes = ['Glioma', 'Meningioma', 'Normal', 'Pituitary']
samples = [num_glioma_samples, num_meningioma_samples, num_normal_samples, num_pituitary_samples]

plt.bar(classes, samples)
plt.xlabel('Tumor Classes')
plt.ylabel('Number of Samples')
plt.title('Number of Samples in Each Class')
plt.show()
# Display images of all four classes
# Define paths to sample images for each class
sample_glioma = os.path.join(glioma_dir, os.listdir(glioma_dir)[0])
sample_meningioma = os.path.join(meningioma_dir, os.listdir(meningioma_dir)[0])
sample_normal = os.path.join(normal_dir, os.listdir(normal_dir)[0])
sample_pituitary = os.path.join(pituitary_dir, os.listdir(pituitary_dir)[0])

# Open and display sample images
fig, axs = plt.subplots(1, 4, figsize=(15, 5))
axs[0].imshow(Image.open(sample_glioma))
axs[0].set_title('Glioma')
axs[0].axis('off')
axs[1].imshow(Image.open(sample_meningioma))
axs[1].set_title('Meningioma')
axs[1].axis('off')
axs[2].imshow(Image.open(sample_normal))
axs[2].set_title('Normal')
axs[2].axis('off')
axs[3].imshow(Image.open(sample_pituitary))
axs[3].set_title('Pituitary')
axs[3].axis('off')
plt.show()
**<font size="4">Split Data</font>**
# Define split ratios
train_ratio = 0.7
test_ratio = 0.2
val_ratio = 0.1
# A function to split data files into train, validation, and test folders
def split_data(source_dir, train_dir, test_dir, val_dir, train_ratio, test_ratio, val_ratio):
    files = os.listdir(source_dir)
    random.shuffle(files)
    total_files = len(files)

    train_split = int(train_ratio * total_files)
    test_split = int(test_ratio * total_files)
    val_split = int(val_ratio * total_files)

    train_files = files[:train_split]
    test_files = files[train_split:(train_split + test_split)]
    val_files = files[(train_split + test_split):]

    for file in train_files:
        src_file = os.path.join(source_dir, file)
        dst_file = os.path.join(train_dir, file)
        if os.path.isfile(src_file):
            shutil.copy(src_file, dst_file)

    for file in test_files:
        src_file = os.path.join(source_dir, file)
        dst_file = os.path.join(test_dir, file)
        if os.path.isfile(src_file):
            shutil.copy(src_file, dst_file)

    for file in val_files:
        src_file = os.path.join(source_dir, file)
        dst_file = os.path.join(val_dir, file)
        if os.path.isfile(src_file):
            shutil.copy(src_file, dst_file)
# Split the data
split_data(glioma_dir, os.path.join(train_dir, 'glioma_tumor'), os.path.join(test_dir, 'glioma_tumor'), os.path.join(val_dir, 'glioma_tumor'), train_ratio, test_ratio, val_ratio)
split_data(meningioma_dir, os.path.join(train_dir, 'meningioma_tumor'), os.path.join(test_dir, 'meningioma_tumor'), os.path.join(val_dir, 'meningioma_tumor'), train_ratio, test_ratio, val_ratio)
split_data(normal_dir, os.path.join(train_dir, 'normal'), os.path.join(test_dir, 'normal'), os.path.join(val_dir, 'normal'), train_ratio, test_ratio, val_ratio)
split_data(pituitary_dir, os.path.join(train_dir, 'pituitary_tumor'), os.path.join(test_dir, 'pituitary_tumor'), os.path.join(val_dir, 'pituitary_tumor'), train_ratio, test_ratio, val_ratio)
# A function to count the number of files in a directory
def count_samples_in_directory(directory):
    return len(os.listdir(directory))
# Count the number of samples in each class and the entire dataset after splitting
train_glioma_samples = count_samples_in_directory(os.path.join(train_dir, 'glioma_tumor'))
test_glioma_samples = count_samples_in_directory(os.path.join(test_dir, 'glioma_tumor'))
val_glioma_samples = count_samples_in_directory(os.path.join(val_dir, 'glioma_tumor'))

train_meningioma_samples = count_samples_in_directory(os.path.join(train_dir, 'meningioma_tumor'))
test_meningioma_samples = count_samples_in_directory(os.path.join(test_dir, 'meningioma_tumor'))
val_meningioma_samples = count_samples_in_directory(os.path.join(val_dir, 'meningioma_tumor'))

train_normal_samples = count_samples_in_directory(os.path.join(train_dir, 'normal'))
test_normal_samples = count_samples_in_directory(os.path.join(test_dir, 'normal'))
val_normal_samples = count_samples_in_directory(os.path.join(val_dir, 'normal'))

train_pituitary_samples = count_samples_in_directory(os.path.join(train_dir, 'pituitary_tumor'))
test_pituitary_samples = count_samples_in_directory(os.path.join(test_dir, 'pituitary_tumor'))
val_pituitary_samples = count_samples_in_directory(os.path.join(val_dir, 'pituitary_tumor'))

# Total samples in each class
total_glioma_samples = train_glioma_samples + test_glioma_samples + val_glioma_samples
total_meningioma_samples = train_meningioma_samples + test_meningioma_samples + val_meningioma_samples
total_normal_samples = train_normal_samples + test_normal_samples + val_normal_samples
total_pituitary_samples = train_pituitary_samples + test_pituitary_samples + val_pituitary_samples

# Total samples in the entire dataset
total_samples = total_glioma_samples + total_meningioma_samples + total_normal_samples + total_pituitary_samples

# Display counts
print("Glioma Samples:")
print(f"Train: {train_glioma_samples}, Test: {test_glioma_samples}, Validation: {val_glioma_samples}, Total: {total_glioma_samples}")
print("\nMeningioma Samples:")
print(f"Train: {train_meningioma_samples}, Test: {test_meningioma_samples}, Validation: {val_meningioma_samples}, Total: {total_meningioma_samples}")
print("\nNormal Samples:")
print(f"Train: {train_normal_samples}, Test: {test_normal_samples}, Validation: {val_normal_samples}, Total: {total_normal_samples}")
print("\nPituitary Samples:")
print(f"Train: {train_pituitary_samples}, Test: {test_pituitary_samples}, Validation: {val_pituitary_samples}, Total: {total_pituitary_samples}")
print("\nTotal Samples in the Entire Dataset:", total_samples)
# A function that plots the distribution of classes of the split data
def plot_sample_distribution(train_samples, val_samples, test_samples, classes):
      # Number of classes
      num_classes = len(classes)

      # Create arrays for the number of samples in each split
      train_counts = [train_samples[class_name] for class_name in classes]
      val_counts = [val_samples[class_name] for class_name in classes]
      test_counts = [test_samples[class_name] for class_name in classes]

      # Create an array of class labels
      x = range(num_classes)

      # Width of a bar
      bar_width = 0.2

      # Create the bar chart
      plt.bar(x, train_counts, bar_width, label='Train', align='center', alpha=0.8)
      plt.bar([i + bar_width for i in x], val_counts, bar_width, label='Validation', align='center', alpha=0.8)
      plt.bar([i + bar_width * 2 for i in x], test_counts, bar_width, label='Test', align='center', alpha=0.8)

      # Set the x-axis labels
      plt.xticks([i + bar_width for i in x], classes, rotation=45)

      # Add labels, title, and legend
      plt.xlabel('Classes')
      plt.ylabel('Number of Samples')
      plt.title('Sample Distribution by Class and Split')
      plt.legend(loc='upper right')

      # Show the plot
      plt.tight_layout()
      plt.show()
# Implementation
classes = ['glioma_tumor', 'meningioma_tumor', 'normal', 'pituitary_tumor']
train_samples = {
    'glioma_tumor': train_glioma_samples,
    'meningioma_tumor': train_meningioma_samples,
    'normal': train_normal_samples,
    'pituitary_tumor': train_pituitary_samples
}
val_samples = {
    'glioma_tumor': val_glioma_samples,
    'meningioma_tumor': val_meningioma_samples,
    'normal': val_normal_samples,
    'pituitary_tumor': val_pituitary_samples
}
test_samples = {
    'glioma_tumor': test_glioma_samples,
    'meningioma_tumor': test_meningioma_samples,
    'normal': test_normal_samples,
    'pituitary_tumor': test_pituitary_samples
}

plot_sample_distribution(train_samples, val_samples, test_samples, classes)
# Randomly check image shape

# Define base directory
base_dir = '/content/brain-tumors-256x256/Data'

# Define subdirectories within train, test, and val
split_subdirectories = ['train', 'test', 'val']

# Randomly select a split directory
selected_split = random.choice(split_subdirectories)

# Define class subdirectories
class_subdirectories = ['glioma_tumor', 'meningioma_tumor', 'normal', 'pituitary_tumor']

# Randomly select a class subdirectory
selected_class = random.choice(class_subdirectories)

# Construct path to the selected image directory
image_dir = os.path.join(base_dir, selected_split, selected_class)

# Check if selected subdirectory exists
if os.path.exists(image_dir):
    # List all image files in the selected image directory
    image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]

    # Randomly select an image file
    if image_files:
        random_image_file = random.choice(image_files)

        # Load randomly selected image
        image = mpimg.imread(random_image_file)

        # Get shape of the image
        image_shape = image.shape
        print(f"Image Shape: {image_shape}")
    else:
        print("No image files found in the selected subdirectory.")
else:
    print(f"The selected subdirectory does not exist: {image_dir}")
# View image as an array
image
**<font size="4">Feature Scaling: Normalization Check</font>**
# Define a function to check normalization

def is_directory_normalized(directory_path):
    """Checks if all images in a directory have been normalized.

      Args:
        directory_path: The path to the directory containing the images.

      Returns:
        True if all images in the directory have been normalized, False otherwise.
    """

    # Iterate through all of the images in the directory
    for filename in os.listdir(directory_path):

        # Check if the file is an image
        if filename.endswith(".jpg") or filename.endswith(".png"):

            # Calculate the mean and standard deviation of the image
            image_path = os.path.join(directory_path, filename)
            image = cv2.imread(image_path)
            mean = np.mean(image)
            std = np.std(image)

            # Check if the mean and standard deviation are close to zero and one
            if abs(mean) >= 0.1 or abs(std - 1.0) >= 0.1:
                return False

    # If all of the images in the directory are normalized, return True
    return True
# Check if all images in the train directory are normalized
is_train_dir_normalized = is_directory_normalized(train_dir)

# Check if all images in the validation directory are normalized
is_val_dir_normalized = is_directory_normalized(val_dir)

# Check if all images in the test directory are normalized
is_test_dir_normalized = is_directory_normalized(test_dir)

# If any of the directories are not normalized, print an error message
if not is_train_dir_normalized or not is_val_dir_normalized or not is_test_dir_normalized:
  print("One or more directories are not normalized.")
  exit(1)

# If all of the directories are normalized, print a success message
print("All directories are normalized.")
# **Data Pre-processing**

# Define data transformations for training
train_transforms = transforms.Compose([
    transforms.RandomRotation(20),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.3333)),
    transforms.ToTensor(),
])



# Define data transformations for validation and test
val_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])
# Create datasets
train_dataset = ImageFolder(train_dir, transform=train_transforms)
val_dataset = ImageFolder(val_dir, transform=val_transforms)
test_dataset = ImageFolder(test_dir, transform=val_transforms)
# Check shape of first samples
assert train_dataset[0][0].shape == val_dataset[0][0].shape == test_dataset[0][0].shape

display(
    train_dataset[0][0].shape,
    val_dataset[0][0].shape,
    test_dataset[0][0].shape
)
# Create a sampler to balance class distribution
sampler = ImbalancedDatasetSampler(train_dataset)

# Set batch size
batch_size = 32

# Create a train Loader using the sampler
train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, pin_memory=True, shuffle=False)

# Create Dataloaders for val and test data
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)
# Iterate through train, val, and test data loaders and print the shape of images and labels
for data_loader, name in [(train_loader, 'Train'), (val_loader, 'Validation'), (test_loader, 'Test')]:
    for images, labels in data_loader:
        print(f"{name} Image shape: {images.shape}, Label Shape: {labels.shape}")
        break
# **Models Development**
## **<font size="4">Summary Class</font>**
class ModelSummary:
    def __init__(self, model):
        self.model = model

    def __str__(self):
        model_summary = str(self.model) + "\n\n"
        total_trainable_params = 0
        total_frozen_params = 0

        for name, parameter in self.model.named_parameters():
            if parameter.requires_grad:
                total_trainable_params += parameter.numel()
                status = "Trainable"
            else:
                total_frozen_params += parameter.numel()
                status = "Frozen"

            model_summary += f"Layer: {name}, Parameters: {parameter.numel()}, Status: {status}\n"

        model_summary += f"Total Trainable Parameters: {total_trainable_params}\n"
        model_summary += f"Total Frozen Parameters: {total_frozen_params}"

        return model_summary
## **<font size="4">Train Class</font>**
class Trainer:
    def __init__(self, model, train_loader, val_loader, criterion=None, optimizer=None, device="cuda", num_epochs=100, patience=5, min_delta=0.001, model_name=None, verbose=False):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion.to(device)
        self.optimizer = optimizer
        self.device = device
        self.num_epochs = num_epochs
        self.patience = patience
        self.min_delta = min_delta
        self.model_name = model_name
        self.verbose = verbose
        self.train_acc_history = []
        self.val_acc_history = []
        self.train_loss_history = []
        self.val_loss_history = []


    def train(self, verbose=False):
            best_val_loss = float('inf')
            early_stopping_counter = 0
            start_time = time.time()

            for epoch in range(self.num_epochs):
                total_correct = 0
                total_samples = 0
                total_loss = 0.0

                if self.verbose:
                    train_data_loader = tqdm(self.train_loader, position=0)
                else:
                    train_data_loader = self.train_loader

                for batch_idx, (inputs, labels) in enumerate(train_data_loader):
                    inputs, labels = inputs.to(self.device), labels.to(self.device)
                    self.optimizer.zero_grad()
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, labels)
                    loss.backward()
                    self.optimizer.step()

                    _, predicted = outputs.max(1)
                    total_samples += labels.size(0)
                    total_correct += (predicted == labels).sum().item()
                    total_loss += loss.item()

                    train_accuracy = total_correct / total_samples
                    train_loss = total_loss / total_samples
                    self.train_acc_history.append(train_accuracy)
                    self.train_loss_history.append(train_loss)

                    if self.verbose:
                        description = f'Epoch [{epoch + 1}/{self.num_epochs}], Batch [{batch_idx + 1}/{len(train_data_loader)}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%'
                        train_data_loader.set_description(description, refresh=True)

                if self.verbose:
                    train_data_loader.close()

                total_correct_val = 0
                total_samples_val = 0
                total_loss_val = 0.0

                # Validation phase
                self.model.eval()


                with torch.no_grad():
                    if self.verbose:
                        val_data_loader = tqdm(self.val_loader, position=0)
                    else:
                        val_data_loader = self.val_loader

                    for batch_idx, (inputs, labels) in enumerate(val_data_loader):
                        inputs, labels = inputs.to(self.device), labels.to(self.device)
                        outputs = self.model(inputs)
                        loss = self.criterion(outputs, labels)
                        total_loss_val += loss.item()

                        _, predicted = outputs.max(1)
                        total_samples_val += labels.size(0)
                        total_correct_val += (predicted == labels).sum().item()

                        val_accuracy = total_correct_val / total_samples_val
                        val_loss = total_loss_val / total_samples_val
                        self.val_acc_history.append(val_accuracy)
                        self.val_loss_history.append(val_loss)

                        if self.verbose:
                            description = f'Epoch [{epoch + 1}/{self.num_epochs}], Batch [{batch_idx + 1}/{len(val_data_loader)}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%'
                            val_data_loader.set_description(description, refresh=True)

                    if self.verbose:
                        val_data_loader.close()

                if val_loss < best_val_loss - self.min_delta:
                    best_val_loss = val_loss
                    early_stopping_counter = 0
                else:
                    early_stopping_counter += 1


                if early_stopping_counter >= self.patience:
                    print("Early stopping triggered. No improvement for {} epochs.".format(self.patience))
                    break

            print("\nTraining completed!")
            end_time = time.time()
            training_time = end_time - start_time
            print(f"Training took {self.format_time(training_time)}\n")

            # Save model
            model_save_path = f'{self.model_name}.pth'
            self.save_model(model_save_path)
            print(f"Model saved at {model_save_path}")




    def train_one_epoch(self, epoch, verbose=None):
        self.model.train()
        if verbose is not None:
            self.verbose = verbose

        total_correct = 0
        total_samples = 0
        total_loss = 0.0

        # Training phase
        if self.verbose:
            train_data_loader = tqdm(self.train_loader, position=0)
        else:
            train_data_loader = self.train_loader

        for batch_idx, (inputs, labels) in enumerate(train_data_loader):
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()

            _, predicted = outputs.max(1)
            total_samples += labels.size(0)
            total_correct += (predicted == labels).sum().item()
            total_loss += loss.item()

            train_accuracy = total_correct / total_samples
            train_loss = total_loss / total_samples
            self.train_acc_history.append(train_accuracy)
            self.train_loss_history.append(train_loss)

            if self.verbose:
                description = f'Epoch [{epoch + 1}/{self.num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%'
                train_data_loader.set_description(description, refresh=True)

        if self.verbose:
            train_data_loader.close()

        total_correct_val = 0
        total_samples_val = 0
        total_loss_val = 0.0

        # Validation phase
        if self.verbose:
            val_data_loader = self.val_loader
        else:
            val_data_loader = self.val_loader

        # Set model to evaluation mode
        self.model.eval()

        with torch.no_grad():
            for batch_idx, (inputs, labels) in enumerate(val_data_loader):
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                total_loss_val += loss.item()

                _, predicted = outputs.max(1)
                total_samples_val += labels.size(0)
                total_correct_val += (predicted == labels).sum().item()

            val_accuracy = total_correct_val / total_samples_val
            val_loss = total_loss_val / total_samples_val
            self.val_acc_history.append(val_accuracy)
            self.val_loss_history.append(val_loss)

            if self.verbose:
                description = f'Epoch [{epoch + 1}/{self.num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%'
                print(description)



    def save_model(self, save_path):
        torch.save(self.model.state_dict(), save_path)


    def format_time(self, seconds):
        minutes, seconds = divmod(seconds, 60)
        hours, minutes = divmod(minutes, 60)
        return f"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds"

## **<font size="4">Training Curve Class</font>**
class TrainCurvePlotter:
    def __init__(self, train_history_file):
        self.train_history_file = train_history_file
        self.train_loss = None
        self.train_acc = None
        self.val_loss = None
        self.val_acc = None
        self.num_epochs = None

    def load_training_history(self):
        with open(self.train_history_file, 'rb') as file:
            history = pickle.load(file)
            self.train_loss = history['train_loss_history']
            self.train_acc = history['train_acc_history']
            self.val_loss = history['val_loss_history']
            self.val_acc = history['val_acc_history']
            train_batches_per_epoch = 68
            val_batches_per_epoch = 10
            self.num_epochs = len(self.train_loss) // train_batches_per_epoch

    def plot(self):
        # Load the training history data
        self.load_training_history()

        train_batches_per_epoch = 68
        val_batches_per_epoch = 10

        # Calculate mean loss and accuracy values per epoch
        train_losses = [np.mean(self.train_loss[i:i+train_batches_per_epoch]) for i in range(0, len(self.train_loss), train_batches_per_epoch)]
        train_accuracies = [np.mean(self.train_acc[i:i+train_batches_per_epoch]) for i in range(0, len(self.train_acc), train_batches_per_epoch)]
        val_losses = [np.mean(self.val_loss[i:i+val_batches_per_epoch]) for i in range(0, len(self.val_loss), val_batches_per_epoch)]
        val_accuracies = [np.mean(self.val_acc[i:i+val_batches_per_epoch]) for i in range(0, len(self.val_acc), val_batches_per_epoch)]

        # Create an epoch range based on the number of epochs
        epochs = range(1, self.num_epochs + 1)

        # Plot training loss and accuracy
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(epochs, train_losses, label='Training Loss')
        plt.plot(epochs, val_losses, label='Validation Loss')
        plt.title('Training and Validation Loss')
        plt.xlabel('Epochs')
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(epochs, train_accuracies, label='Training Accuracy')
        plt.plot(epochs, val_accuracies, label='Validation Accuracy')
        plt.title('Training and Validation Accuracy')
        plt.xlabel('Epochs')
        plt.legend()

        plt.tight_layout()
        plt.show()
## **<font size="4">Evaluate Model Class</font>**
class ModelEvaluator:
    def __init__(self, model, dataloader, device):
        self.model = model
        self.dataloader = dataloader
        self.device = device

    def evaluate(self):
        self.model.eval()

        total_correct = 0
        total_samples = 0

        for inputs, labels in self.dataloader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            outputs = self.model(inputs)

            _, predicted = outputs.max(1)
            total_samples += labels.size(0)
            total_correct += (predicted == labels).sum().item()

        accuracy = total_correct / total_samples

        return accuracy
## **<font size="4">Cross Validation & Hyperparameters Tuning Class</font>**
class CrossValidatedHyperparameterTuner:
    def __init__(self, model_class, train_loader, model_param_grid, trainer_param_grid,
                 num_folds=5, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):
        self.model_class = model_class
        self.train_loader = train_loader
        self.model_param_grid = model_param_grid
        self.trainer_param_grid = trainer_param_grid
        self.device = device
        self.num_folds = num_folds
        self.verbose = verbose

        self.best_accuracy = 0
        self.best_std = float('inf')
        self.best_params = {}
        self.best_model = None

    def tune(self):
        # Create Stratified K-Fold object
        skf = StratifiedKFold(n_splits=self.num_folds, shuffle=True, random_state=42)

        # Iterate over the model hyperparameter grid
        for model_params in ParameterGrid(self.model_param_grid):
            # Iterate over the trainer hyperparameter grid
            for trainer_params in ParameterGrid(self.trainer_param_grid):
                # Create a list to store fold accuracies
                fold_accuracies = []

                # Train the model on each fold
                for fold, (train_index, val_index) in enumerate(skf.split(range(len(self.train_loader.dataset.targets)), y=self.train_loader.dataset.targets)):

                    # Create a train loader for the current fold
                    fold_train_loader = DataLoader(
                        dataset=self.train_loader.dataset,
                        batch_size=self.train_loader.batch_size,
                        sampler=SubsetRandomSampler(train_index),
                        shuffle=False
                    )

                    # Create a validation loader for the current fold
                    fold_val_loader = DataLoader(
                        dataset=self.train_loader.dataset,
                        batch_size=self.train_loader.batch_size,
                        sampler=SubsetRandomSampler(val_index),
                        shuffle=False
                    )

                    # Print the fold number
                    print(f"Fold: {fold + 1}")

                    # Create a new model instance with the current model hyperparameters
                    model = self.model_class(**model_params).to(self.device)

                    # Unpack the optimizer and its parameters from trainer_params
                    optimizer, optimizer_params = trainer_params['optimizer']

                    # Create the optimizer instance
                    optimizer_instance = optimizer(model.parameters(), **optimizer_params)

                    # Create a new Trainer instance
                    trainer = Trainer(
                        model=model,
                        train_loader=fold_train_loader,
                        val_loader=fold_val_loader,
                        criterion=nn.CrossEntropyLoss(weight=torch.Tensor(class_weights)).to(self.device),
                        optimizer=optimizer_instance,
                        device=self.device,
                        num_epochs=trainer_params['num_epochs'],
                        verbose=True,
                        )


                    # Create a fold iterator
                    fold_iterator = range(trainer_params['num_epochs'])

                    # Train the model using the training data for the current fold
                    for epoch in fold_iterator:
                        trainer.train_one_epoch(epoch, verbose=True)

                    # Evaluate the model on the validation set of the current fold
                    evaluator = ModelEvaluator(model=model, dataloader=fold_val_loader, device=self.device)
                    accuracy = evaluator.evaluate() * 100

                    # Print the fold accuracy
                    print(f"Fold {fold + 1} Accuracy: {accuracy:.2f}%")

                    # Append the fold accuracy to the list of fold accuracies
                    fold_accuracies.append(accuracy)

                # Calculate and print the mean and standard deviation of the fold accuracies
                mean = np.mean(fold_accuracies)
                std = np.std(fold_accuracies)

                # Calculate a score that combines mean accuracy and standard deviation
                # Higher score for higher mean accuracy and lower standard deviation
                score = mean - std

                print(f"Model Parameters: {model_params}, Trainer Parameters: {trainer_params}")
                print(f"Fold Accuracies: {fold_accuracies}")
                print(f"Mean Accuracy: {mean:.2f}%, Std Accuracy: {std:.2f}, Score: {score:.2f}")

                if mean > self.best_accuracy or (mean == self.best_accuracy and std < self.best_std):
                    self.best_accuracy = mean
                    self.best_std = std
                    self.best_params = (model_params, trainer_params)
                    self.best_model = model



        print("Best hyperparameters:", self.best_params)
        print("Best mean accuracy:", self.best_accuracy)
        print("Best standard deviation:", self.best_std)
        print("Best model:", self.best_model)
## **<font size="4">Model Predictor Class</font>**
class ModelPredictor:
    def __init__(self, model, dataloader, device):
        self.model = model
        self.dataloader = dataloader
        self.device = device

    def predict(self):
        predictions = []
        true_labels = []

        with torch.no_grad():
            for inputs, labels in self.dataloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                predicted = outputs.argmax(dim=1)

                predictions.extend(predicted.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())

        predictions = np.array(predictions)
        true_labels = np.array(true_labels)

        return predictions, true_labels


    def plot_predictions(self):
        predictions, true_labels = self.predict()

        # Create a grouped bar chart
        classes = np.unique(true_labels)
        class_labels = [str(cls) for cls in classes]

        width = 0.35
        ind = np.arange(len(classes))

        true_counts = [0] * len(classes)
        predicted_counts = [0] * len(classes)

        for true, pred in zip(true_labels, predictions):
            true_counts[true] += 1
            predicted_counts[pred] += 1

        fig, ax = plt.subplots(figsize=(10, 6))
        p1 = ax.bar(ind - width / 2, true_counts, width, label='True Labels')
        p2 = ax.bar(ind + width / 2, predicted_counts, width, label='Predicted Labels')

        ax.set_xlabel('Classes')
        ax.set_ylabel('Counts')
        ax.set_title('True vs. Predicted Labels')
        ax.set_xticks(ind)
        ax.set_xticklabels(class_labels)
        ax.legend()

        # Annotate the bars with counts
        for i in range(len(classes)):
            ax.annotate(str(true_counts[i]), (ind[i] - width / 2, true_counts[i] + 1), ha='center', color='blue')
            ax.annotate(str(predicted_counts[i]), (ind[i] + width / 2, predicted_counts[i] + 1), ha='center', color='orange')

        plt.show()

## **<font size="4">Model Performance Class</font>**
class PerformanceMetrics:
    def __init__(self, true_labels, predicted_labels, class_names):
        self.true_labels = true_labels
        self.predicted_labels = predicted_labels
        self.class_names = class_names

    def calculate_accuracy(self):
        accuracy = accuracy_score(self.true_labels, self.predicted_labels)
        return accuracy * 100

    def calculate_confusion_matrix(self):
        return confusion_matrix(self.true_labels, self.predicted_labels)

    def calculate_specificity(self):
        conf_matrix = self.calculate_confusion_matrix()
        true_negatives = conf_matrix[0, 0]
        false_positives = conf_matrix[0, 1]
        specificity = true_negatives / (true_negatives + false_positives) * 100
        return specificity

    def calculate_sensitivity(self):
        conf_matrix = self.calculate_confusion_matrix()
        true_positives = conf_matrix[1, 1]
        false_negatives = conf_matrix[1, 0]
        sensitivity = true_positives / (true_positives + false_negatives) * 100
        return sensitivity

    def calculate_precision(self):
        conf_matrix = self.calculate_confusion_matrix()
        true_positives = conf_matrix[1, 1]
        false_positives = conf_matrix[0, 1]
        precision = true_positives / (true_positives + false_positives) * 100
        return precision

    def generate_classification_report(self):
        return classification_report(self.true_labels, self.predicted_labels, target_names=self.class_names)

    def plot_heatmap(self):
        conf_matrix = self.calculate_confusion_matrix()
        plt.figure(figsize=(8, 6))
        sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="YlGnBu", xticklabels=self.class_names, yticklabels=self.class_names)
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix Heatmap')
        plt.show()
# Set the random seed for Python's built-in random module
random.seed(0)

# Set the random seed for NumPy
np.random.seed(0)

# Set the random seed for PyTorch
torch.manual_seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Calculate class weights based on the distribution of samples
class_counts = [train_dataset.targets.count(c) for c in range(len(classes))]
total_samples = sum(class_counts)
class_weights = [total_samples / (count * len(classes)) for count in class_counts]
## **Baseline Model**
**<font size="4">Setting up Model Architecture</font>**
class BaseNet(nn.Module):
    def __init__(self, num_classes=4, conv1_out_channels=32, conv2_out_channels=64, conv3_out_channels=128,
                 fc1_out_features=256, dropout_prob=0.5):
        super(BaseNet, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(3, conv1_out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(conv1_out_channels, conv2_out_channels, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(conv2_out_channels, conv3_out_channels, kernel_size=3, padding=1)

        # Pooling layers
        self.pool = nn.MaxPool2d(2, 2)

        # Fully connected layers
        self.fc1 = nn.Linear(conv3_out_channels * (224 // (2**3)) * (224 // (2**3)), fc1_out_features)
        self.fc2 = nn.Linear(fc1_out_features, num_classes)

        # Dropout layers
        self.dropout1 = nn.Dropout(dropout_prob)
        self.dropout2 = nn.Dropout(dropout_prob)

    def forward(self, x):
        # Convolutional layers
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))

        # Flatten the feature map
        x = x.view(-1, self.conv3.out_channels * (224 // (2**3)) * (224 // (2**3)))

        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = self.fc2(x)
        x = self.dropout2(x)

        return x
**<font size="4">Getting Model Summary</font>**
# Define baseline model
baseline_model = BaseNet()

# Instantiate ModelSummary class
model_summary = ModelSummary(baseline_model)

# Display model summary
print(model_summary)
**<font size="4">Cross Validation & Hyperparameter Tuning</font>**
# Define hyperparameter grids
model_param_grid = {
    #'conv1_out_channels': [64, 128],
    #'conv2_out_channels': [128, 256],
    'fc1_out_features': [256, 512],
    'dropout_prob': [0.5, 0.7],
}

trainer_param_grid = {
    'num_epochs': [10, 20],
    'optimizer': [(torch.optim.Adam, {'lr': 0.0001}), (torch.optim.SGD, {'lr': 0.0001})],
}

# Instantiate CrossValidatedHyperparameterTuner class
cv_tuner = CrossValidatedHyperparameterTuner(
    model_class=BaseNet,
    train_loader=train_loader,
    model_param_grid=model_param_grid,
    trainer_param_grid=trainer_param_grid,
    num_folds=5,
    verbose=True,
)

# Tune model and trainer parameters while performing cross-validation
cv_tuner.tune()
**<font size="4">Training the Model</font>**
# Best hyperparameters
best_model_params = {'dropout_prob': 0.5, 'fc1_out_features': 512}
best_optimizer_params = {'num_epochs': 20, 'optimizer': (torch.optim.Adam, {'lr': 0.0001})}

# Initialize model using the best model hyperparameters
baseline_model = BaseNet(**best_model_params)

# Extract the optimizer and optimizer parameters
best_optimizer, optimizer_params = best_optimizer_params['optimizer']
lr = optimizer_params['lr']

# Create an optimizer instance with the learning rate
optimizer_instance = best_optimizer(baseline_model.parameters(), lr=lr)

# Instantiate Trainer with the best model and optimizer
trainer = Trainer(
    model=baseline_model,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=nn.CrossEntropyLoss(weight=torch.Tensor(class_weights)).to(device),
    optimizer=optimizer_instance,
    device=device,
    num_epochs=best_optimizer_params['num_epochs'],
    model_name='basenet',
    verbose=True
)

# Train the best model
trainer.train()
# Get link to download saved model
file_dir = 'basenet.pth'
file_link = FileLink(file_dir)
file_link
def save_training_history(history, filename):
    # Save the history to the specified file
    with open(filename, 'wb') as file:
        pickle.dump(history, file)
# Save training history for trainer1
history1 = {
    'train_loss_history': trainer.train_loss_history,
    'val_loss_history': trainer.val_loss_history,
    'train_acc_history': trainer.train_acc_history,
    'val_acc_history': trainer.val_acc_history,
}
save_training_history(history1, 'training_history_baseline_model.pkl')
**<font size="4">Plot Training Curves</font>**
# Instantiate TrainCurvePlotter with saved training history
plotter = TrainCurvePlotter('training_history_baseline_model.pkl')

# Load training history
plotter.load_training_history()

# Plot training curves
plotter.plot()
**<font size="4">Load Saved Model</font>**
# Set path to saved model checkpoint
model_path = 'basenet.pth'

# Initialize baseline model
baseline_model = BaseNet(fc1_out_features=512, dropout_prob=0.5).to(device)

# Load the model
baseline_model.load_state_dict(torch.load(model_path))
**<font size="4">Baseline Model Evaluation</font>**
# Instantiate ModelEvaluator
evaluator = ModelEvaluator(baseline_model, test_loader, device)

# Get test accuracy
test_accuracy = evaluator.evaluate()

print(f'Test Accuracy: {test_accuracy * 100:.2f}%')
**<font size="4">Make Predictions</font>**
# Instantiate ModelPredictor
predictor = ModelPredictor(baseline_model, test_loader, device)

# Predict the class labels of all the images in test loader
predictions, true_labels = predictor.predict()

# Print the accuracy of the model on the test dataset
accuracy = (predictions == true_labels).mean()
print(f"Accuracy: {accuracy * 100:.2f}%")
# Get a random batch from the test loader
random_batch = random.choice(test_loader.dataset)

# Extract the image and label from the random batch
image, label = random_batch

# Add a batch dimension to the image
image = image.unsqueeze(0).to(device)  # Add a batch dimension

# Make predictions for the random image
outputs = baseline_model(image)

# Get the predicted class label
predicted_class_label = outputs[0].argmax().item()

# Get the class name corresponding to the predicted and true labels
predicted_class_name = classes[predicted_class_label]
true_class_name = classes[label]

# Display results
print(f"Predicted class label: {predicted_class_label}")
print(f"Predicted class name: {predicted_class_name}")
print(f"True class label: {label}")
print(f"True class name: {true_class_name}")
# Plot the predictions vs. true labels
predictor.plot_predictions()
**<font size="4">Getting Model Performance</font>**
# Instantiate performance metrics
metrics = PerformanceMetrics(true_labels, predictions, classes)

# Calculate accuracy
accuracy = metrics.calculate_accuracy()
print(f"Accuracy: {accuracy:.2f}%, \n")

# Calculate specificity
specificity = metrics.calculate_specificity()
print(f"Specificity: {specificity:.2f}%, \n")

# Calculate sensitivity
sensitivity = metrics.calculate_sensitivity()
print(f"Sensitivity: {sensitivity:.2f}%, \n")

# Calculate precision
precision = metrics.calculate_precision()
print(f"Precision: {precision:.2f}%, \n")

print("*" * 100)

# Display classification report
report = metrics.generate_classification_report()
print(report)
# Display heatmap
metrics.plot_heatmap()
**<font size="4">Store Results</font>**
# Create an empty dictionary to save metrics
model_results = {}
# Store the results of the baseline model
baseline_results = {
    "Accuracy": accuracy,
    "Specificity": specificity,
    "Sensitivity": sensitivity,
    "Precision": precision,
    "Classification Report": classification_report
}

# Add baseline results to the dictionary
model_results["Baseline Model"] = baseline_results
## **Model 1 - Pretrained Model from Pytorch Hub: ResNet50**
**<font size="4">Setting Up Model Architecture</font>**
class ResNet50(nn.Module):
    def __init__(self, num_classes=4, weights='ResNet50_Weights', freeze_layers=False):
        super(ResNet50, self).__init__()

        # Load a pretrained ResNet-50 model with specified weights
        if weights == 'ResNet50_Weights':
            resnet50 = models.resnet50(weights=ResNet50_Weights)

        else:
            resnet50 = models.resnet50()

        if freeze_layers:
            # Freeze all layers except the custom classification layers
            for param in resnet50.parameters():
                param.requires_grad = True

        # Remove the final classification layer
        self.resnet50 = nn.Sequential(*list(resnet50.children())[:-2])

        # Add custom classification layers
        self.avgpool = nn.AdaptiveAvgPool2d(1)  # Global Average Pooling
        self.fc = nn.Linear(2048, num_classes)

    def forward(self, x):
        features = self.resnet50(x)
        x = self.avgpool(features)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
**<font size="4">Getting Model Summary</font>**
# Define model 1
model_1 = ResNet50()

# Instantiate ModelSummary class
model_summary = ModelSummary(model_1)

# Display model summary
print(model_summary)
**<font size="4">Cross Validation & Hyperparameter Tuning</font>**
 # Define hyperparameter grid
model_param_grid = {
    'freeze_layers': [True, False],  # Freeze or unfreeze layers
    'weights': ['ResNet50_Weights', None],  # Pretrained weights or None
}

trainer_param_grid = {
    'num_epochs': [10, 20],
    'optimizer': [(torch.optim.Adam, {'lr': 0.0001}), (torch.optim.SGD, {'lr': 0.001})],
}

# Instantiate CrossValidatedHyperparameterTuner
cv_tuner = CrossValidatedHyperparameterTuner(
    model_class=ResNet50,
    train_loader=train_loader,
    model_param_grid=model_param_grid,
    trainer_param_grid=trainer_param_grid,
    num_folds=5,
    verbose=True,
)

# Tune model and trainer parameters while performing cross-validation
cv_tuner.tune()
**<font size="4">Training the model</font>**
# Best hyperparameters
best_model_params = {'freeze_layers': False,'weights': 'ResNet50_Weights'}
best_optimizer_params = {'num_epochs': 20, 'optimizer': (torch.optim.Adam, {'lr': 0.0001})}

# Initialize model using the best model hyperparameters
model_1 = ResNet50(**best_model_params)

# Extract the optimizer and optimizer parameters
best_optimizer, optimizer_params = best_optimizer_params['optimizer']
lr = optimizer_params['lr']

# Create an optimizer instance with the learning rate
optimizer_instance = best_optimizer(model_1.parameters(), lr=lr)

# Instantiate Trainer with the best model and optimizer
trainer = Trainer(
    model=model_1,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=nn.CrossEntropyLoss(weight=torch.Tensor(class_weights)).to(device),
    optimizer=optimizer_instance,
    device=device,
    num_epochs=best_optimizer_params['num_epochs'],
    model_name='resnet50',
    verbose=True
)

# Train the best model
trainer.train()
# Get link to download saved model
file_dir = 'resnet50.pth'
file_link = FileLink(file_dir)
file_link
# Save training history for trainer 2
history2 = {
    'train_loss_history': trainer.train_loss_history,
    'val_loss_history': trainer.val_loss_history,
    'train_acc_history': trainer.train_acc_history,
    'val_acc_history': trainer.val_acc_history,
}
save_training_history(history2, 'training_history_model1.pkl')
**<font size="4">Plot Training Curves</font>**
# Instantiate TrainCurvePlotter with saved training history
plotter = TrainCurvePlotter('training_history_model1.pkl')

# Load training history
plotter.load_training_history()

# Plot training curves
plotter.plot()
**<font size="4">Load Saved Model</font>**
# Set path to saved model checkpoint
model_path = 'resnet50.pth'

# Initialize model
model_1 = ResNet50(weights='ResNet50_Weights').to(device)

# Load the model
model_1.load_state_dict(torch.load(model_path))
**<font size="4">Model 1 Evaluation</font>**
# Instantiate ModelEvaluator class
evaluator = ModelEvaluator(model_1, test_loader, device)

# Get test accuracy
test_accuracy = evaluator.evaluate()

print(f'Test Accuracy: {test_accuracy * 100:.2f}%')
**<font size="4">Make Predictions</font>**
# Instantiate ModelPredictor
predictor = ModelPredictor(model_1, test_loader, device)

# Predict the class labels of all the images test loader
predictions, true_labels = predictor.predict()

# Print the accuracy of the model on the test dataset
accuracy = (predictions == true_labels).mean()
print(f"Accuracy: {accuracy * 100:.2f}%")
# Get a random batch from the test loader
random_batch = random.choice(test_loader.dataset)

# Extract the image and label from the random batch
image, label = random_batch

# Add a batch dimension to the image
image = image.unsqueeze(0).to(device)  # Add a batch dimension

# Make predictions for the random image
outputs = model_1(image)

# Get the predicted class label
predicted_class_label = outputs[0].argmax().item()

# Get the class name corresponding to the predicted and true labels
predicted_class_name = classes[predicted_class_label]
true_class_name = classes[label]

# Display results
print(f"Predicted class label: {predicted_class_label}")
print(f"Predicted class name: {predicted_class_name}")
print(f"True class label: {label}")
print(f"True class name: {true_class_name}")
# Get a random batch from the test loader
random_batch = random.choice(test_loader.dataset)

# Extract the image and label from the random batch
image, label = random_batch

# Add a batch dimension to the image
image = image.unsqueeze(0).to(device)  # Add a batch dimension

# Make predictions for the random image
outputs = model_1(image)

# Get the predicted class label
predicted_class_label = outputs[0].argmax().item()

# Get the class name corresponding to the predicted and true labels
predicted_class_name = classes[predicted_class_label]
true_class_name = classes[label]

# Display results
print(f"Predicted class label: {predicted_class_label}")
print(f"Predicted class name: {predicted_class_name}")
print(f"True class label: {label}")
print(f"True class name: {true_class_name}")
# Plot the predictions vs. true labels
predictor.plot_predictions()
**<font size="4">Getting Model Performance</font>**
# Instantiate performance metrics
metrics = PerformanceMetrics(true_labels, predictions, classes)

# Calculate accuracy
accuracy = metrics.calculate_accuracy()
print(f"Accuracy: {accuracy:.2f}%, \n")

# Calculate specificity
specificity = metrics.calculate_specificity()
print(f"Specificity: {specificity:.2f}%, \n")

# Calculate sensitivity
sensitivity = metrics.calculate_sensitivity()
print(f"Sensitivity: {sensitivity:.2f}%, \n")

# Calculate precision
precision = metrics.calculate_precision()
print(f"Precision: {precision:.2f}%, \n")

print("*" * 100)

# Display classification report
report = metrics.generate_classification_report()
print(report)
# Display heatmap
metrics.plot_heatmap()
**<font size="4">Store Results</font>**
# Store the results of model 1
model_1_results = {
    "Accuracy": accuracy,
    "Specificity": specificity,
    "Sensitivity": sensitivity,
    "Precision": precision,
    "Classification Report": classification_report
}

# Add model 1 results to the dictionary
model_results["Model One"] = model_1_results
## **Model 2 - Pretrained Model from Pytorch Hub: VGG11**

**<font size="4">Setting Up Model Architecture</font>**
class VGG11WithBN(nn.Module):
    def __init__(self, num_classes=4, weights='VGG11_BN_Weights', freeze_layers=False):
        super(VGG11WithBN, self).__init__()

        # Load a pretrained VGG11 model with specified weights
        if weights == 'VGG11_BN_Weights':
            vgg11 = models.vgg11_bn(weights=VGG11_BN_Weights)

        else:
            vgg11 = models.vgg11_bn()

        if freeze_layers:
            # Freeze all layers except the custom classification layers
            for param in vgg11.parameters():
                param.requires_grad = True

        # Remove the final classification layer
        self.features = vgg11.features

        # Add custom classification layers
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
**<font size="4">Getting Model Summary</font>**
# Define model 2
model_2 = VGG11WithBN()

# Instantiate ModelSummary
model_summary = ModelSummary(model_2)

# Print model summary
print(model_summary)
**<font size="4">Cross Validation & Hyperparameter Tuning</font>**
# Define hyperparameter grid
model_param_grid = {
    'freeze_layers': [True, False],  # Freeze or unfreeze layers
    'weights': ['VGG11_BN_Weights', None],  # Pretrained weights or None
}

trainer_param_grid = {
    'num_epochs': [10, 20],
    'optimizer': [(torch.optim.Adam, {'lr': 0.0001}), (torch.optim.SGD, {'lr': 0.001})],
}

# Instantiate CrossValidatedHyperparameterTuner
cv_tuner = CrossValidatedHyperparameterTuner(
    model_class=VGG11WithBN,
    train_loader=train_loader,
    model_param_grid=model_param_grid,
    trainer_param_grid=trainer_param_grid,
    num_folds=5,
    verbose=True,
)

# Tune model and trainer parameters while performing cross-validation
cv_tuner.tune()
**<font size="4">Training the model</font>**
# Best hyperparameters
best_model_params = {'freeze_layers': False,'weights': 'VGG11_BN_Weights'}
best_optimizer_params = {'num_epochs': 20, 'optimizer': (torch.optim.Adam, {'lr': 0.0001})}

# Initialize model using the best model hyperparameters
model_2 = VGG11WithBN(**best_model_params)

# Extract the optimizer and optimizer parameters
best_optimizer, optimizer_params = best_optimizer_params['optimizer']
lr = optimizer_params['lr']

# Create an optimizer instance with the learning rate
optimizer_instance = best_optimizer(model_2.parameters(), lr=lr)

# Instantiate Trainer with the best model and optimizer
trainer = Trainer(
    model=model_2,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=nn.CrossEntropyLoss(weight=torch.Tensor(class_weights)).to(device),
    optimizer=optimizer_instance,
    device=device,
    num_epochs=best_optimizer_params['num_epochs'],
    model_name='vgg11bn',
    verbose=True
)

# Train the best model
trainer.train()
# Get link to download saved model
file_dir = 'vgg11bn.pth'
file_link = FileLink(file_dir)
file_link
# Save the training history for trainer 3
history3 = {
    'train_loss_history': trainer.train_loss_history,
    'val_loss_history': trainer.val_loss_history,
    'train_acc_history': trainer.train_acc_history,
    'val_acc_history': trainer.val_acc_history,
}
save_training_history(history3, 'training_history_model2.pkl')
**<font size="4">Plot Training Curves</font>**
# Create a TrainCurvePlotter for your saved training history
plotter = TrainCurvePlotter('training_history_model2.pkl')

# Load the training history
plotter.load_training_history()

# Plot the training curves
plotter.plot()
**<font size="4">Load Saved Model</font>**
# Set path to saved model checkpoint
model_path = 'vgg11bn.pth'

# Initialize model
model_2 = VGG11WithBN(weights='VGG11_BN_Weights').to(device)

# Load the model
model_2.load_state_dict(torch.load(model_path))
**<font size="4">Model 2 Evaluation</font>**
# Instantiate ModelEvaluator
evaluator = ModelEvaluator(model_2, test_loader, device)

# Get test accuracy
test_accuracy = evaluator.evaluate()

print(f'Test Accuracy: {test_accuracy * 100:.2f}%')
**<font size="4">Make Predictions</font>**
# Instantiate ModelPredictor
predictor = ModelPredictor(model_2, test_loader, device)

# Predict the class labels of all the images test loader
predictions, true_labels = predictor.predict()

# Print the accuracy of the model on the test dataset
accuracy = (predictions == true_labels).mean()
print(f"Accuracy: {accuracy * 100:.2f}%")
# Get a random batch from the test loader
random_batch = random.choice(test_loader.dataset)

# Extract the image and label from the random batch
image, label = random_batch

# Add a batch dimension to the image
image = image.unsqueeze(0).to(device)  # Add a batch dimension

# Make predictions for the random image
outputs = model_2(image)

# Get the predicted class label
predicted_class_label = outputs[0].argmax().item()

# Get the class name corresponding to the predicted and true labels
predicted_class_name = classes[predicted_class_label]
true_class_name = classes[label]

# Display results
print(f"Predicted class label: {predicted_class_label}")
print(f"Predicted class name: {predicted_class_name}")
print(f"True class label: {label}")
print(f"True class name: {true_class_name}")
# Get a random batch from the test loader
random_batch = random.choice(test_loader.dataset)

# Extract the image and label from the random batch
image, label = random_batch

# Add a batch dimension to the image
image = image.unsqueeze(0).to(device)  # Add a batch dimension

# Make predictions for the random image
outputs = model_2(image)

# Get the predicted class label
predicted_class_label = outputs[0].argmax().item()

# Get the class name corresponding to the predicted and true labels
predicted_class_name = classes[predicted_class_label]
true_class_name = classes[label]

# Display results
print(f"Predicted class label: {predicted_class_label}")
print(f"Predicted class name: {predicted_class_name}")
print(f"True class label: {label}")
print(f"True class name: {true_class_name}")
# Plot the predictions vs. true labels
predictor.plot_predictions()
**<font size="4">Getting Model Performance</font>**
# Instantiate performance metrics
metrics = PerformanceMetrics(true_labels, predictions, classes)

# Calculate accuracy
accuracy = metrics.calculate_accuracy()
print(f"Accuracy: {accuracy:.2f}%, \n")

# Calculate specificity
specificity = metrics.calculate_specificity()
print(f"Specificity: {specificity:.2f}%, \n")

# Calculate sensitivity
sensitivity = metrics.calculate_sensitivity()
print(f"Sensitivity: {sensitivity:.2f}%, \n")

# Calculate precision
precision = metrics.calculate_precision()
print(f"Precision: {precision:.2f}%, \n")

print("*" * 100)

# Display classification report
report = metrics.generate_classification_report()
print(report)
# Display heatmap
metrics.plot_heatmap()
**<font size="4">Store Results</font>**
# Store the results of model 2
model_2_results = {
    "Accuracy": accuracy,
    "Specificity": specificity,
    "Sensitivity": sensitivity,
    "Precision": precision,
    "Classification Report": classification_report
}

# Add baseline results to the dictionary
model_results["Model Two"] = model_2_results
## **Model 3 - Tuned ResNet50: Same Archictecture, Different LR**
**<font size="4">Training the Model</font>**
# Set model parameters
model_params = {
    'freeze_layers': False,
    'weights': 'ResNet50_Weights'
}

# Define hyperparameters
num_epochs = 50
learning_rate = 0.00006
patience = 10

# Initialize model with updated hyperparameters
model_3 = ResNet50(**model_params)

# Define loss function
criterion = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights)).to(device)

# Define optimizer
optimizer = optim.Adam(model_3.parameters(), lr=learning_rate)

# Instantiate Trainer with the updated model and optimizer
trainer = Trainer(
    model=model_3,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer,
    device=device,
    num_epochs=num_epochs,
    patience=patience,
    model_name='resnet50_tuned',
    verbose=True
)

# Train the model with the updated hyperparameters
trainer.train()
# Get link to download saved model
file_dir = 'resnet50_tuned.pth'
file_link = FileLink(file_dir)
file_link
# Save training history for trainer 4
history4 = {
    'train_loss_history': trainer.train_loss_history,
    'val_loss_history': trainer.val_loss_history,
    'train_acc_history': trainer.train_acc_history,
    'val_acc_history': trainer.val_acc_history,
}
save_training_history(history4, 'training_history_model3.pkl')
**<font size="4">Plot Training Curves</font>**
# Instantiate TrainCurvePlotter with saved training history
plotter = TrainCurvePlotter('training_history_model3.pkl')

# Load training history
plotter.load_training_history()

# Plot training curves
plotter.plot()
**<font size="4">Load Saved Model</font>**
# Set path to saved model checkpoint
model_path = 'resnet50_tuned.pth'

# Initialize model
model_3 = ResNet50(weights='ResNet50_Weights').to(device)

# Load the model
model_3.load_state_dict(torch.load(model_path))
**<font size="4">Model 3 Evaluation</font>**
# Instantiate ModelEvaluator
evaluator = ModelEvaluator(model_3, test_loader, device)

# Get test accuracy
test_accuracy = evaluator.evaluate()

print(f'Test Accuracy: {test_accuracy * 100:.2f}%')
**<font size="4">Make Predictions</font>**
# Instantiate ModelPredictor
predictor = ModelPredictor(model_3, test_loader, device)

# Predict the class labels of all the images test loader
predictions, true_labels = predictor.predict()

# Print the accuracy of the model on the test dataset
accuracy = (predictions == true_labels).mean()
print(f"Accuracy: {accuracy * 100:.2f}%")
# Get a random batch from the test loader
random_batch = random.choice(test_loader.dataset)

# Extract the image and label from the random batch
image, label = random_batch

# Add a batch dimension to the image
image = image.unsqueeze(0).to(device)  # Add a batch dimension

# Make predictions for the random image
outputs = model_3(image)

# Get the predicted class label
predicted_class_label = outputs[0].argmax().item()

# Get the class name corresponding to the predicted and true labels
predicted_class_name = classes[predicted_class_label]
true_class_name = classes[label]

# Display results
print(f"Predicted class label: {predicted_class_label}")
print(f"Predicted class name: {predicted_class_name}")
print(f"True class label: {label}")
print(f"True class name: {true_class_name}")
# Get a random batch from the test loader
random_batch = random.choice(test_loader.dataset)

# Extract the image and label from the random batch
image, label = random_batch

# Add a batch dimension to the image
image = image.unsqueeze(0).to(device)  # Add a batch dimension

# Make predictions for the random image
outputs = model_3(image)

# Get the predicted class label
predicted_class_label = outputs[0].argmax().item()

# Get the class name corresponding to the predicted and true labels
predicted_class_name = classes[predicted_class_label]
true_class_name = classes[label]

# Display results
print(f"Predicted class label: {predicted_class_label}")
print(f"Predicted class name: {predicted_class_name}")
print(f"True class label: {label}")
print(f"True class name: {true_class_name}")
# Plot the predictions vs. true labels
predictor.plot_predictions()
**<font size="4">Getting Model Performance</font>**
# Instantiate performance metrics
metrics = PerformanceMetrics(true_labels, predictions, classes)

# Calculate accuracy
accuracy = metrics.calculate_accuracy()
print(f"Accuracy: {accuracy:.2f}%, \n")

# Calculate specificity
specificity = metrics.calculate_specificity()
print(f"Specificity: {specificity:.2f}%, \n")

# Calculate sensitivity
sensitivity = metrics.calculate_sensitivity()
print(f"Sensitivity: {sensitivity:.2f}%, \n")

# Calculate precision
precision = metrics.calculate_precision()
print(f"Precision: {precision:.2f}%, \n")

print("*" * 100)

# Display classification report
report = metrics.generate_classification_report()
print(report)
# Display heatmap
metrics.plot_heatmap()
**<font size="4">Store Results</font>**
# Load the model_results dictionary from the file
with open('/kaggle/input/modelresults/model_results (2).pkl', 'rb') as file:
    model_results = pickle.load(file)
# Store the results of model 1
model_3_results = {
    "Accuracy": accuracy,
    "Specificity": specificity,
    "Sensitivity": sensitivity,
    "Precision": precision,
    "Classification Report": classification_report
}

# Add baseline results to the dictionary
model_results["Model Three"] = model_3_results
# Remove the 'Classification Report' entry from each model's results
model_results = {model: {key: value for key, value in results.items() if key != 'Classification Report'} for model, results in model_results.items()}
# Display results
model_results
model_results = {
    'Baseline Model': {'Accuracy': 83.63047001620745,
                       'Specificity': 83.73493975903614,
                       'Sensitivity': 96.81528662420382,
                       'Precision': 84.91620111731844},
    'Model One': {'Accuracy': 95.13776337115073,
                    'Specificity': 97.76536312849163,
                    'Sensitivity': 93.67816091954023,
                    'Precision': 97.60479041916167},
    'Model Two': {'Accuracy': 93.67909238249594,
                    'Specificity': 96.045197740113,
                    'Sensitivity': 92.09039548022598,
                    'Precision': 95.88235294117648},
    'Model Three': {'Accuracy': 95.62398703403565,
                     'Specificity': 96.61016949152543,
                     'Sensitivity': 96.13259668508287,
                     'Precision': 96.66666666666667}}
# Save the model_results dictionary to a file
with open('model_results.pkl', 'wb') as file:
    pickle.dump(model_results, file)
# **Models Comparison**
class ModelComparison:
    def __init__(self, model_results):
        self.model_results = model_results

    def plot_metric(self, metric_name, plot_title):
        data = []
        for model, metrics in self.model_results.items():
            data.append({'Model': model, metric_name: metrics[metric_name]})

        df = pd.DataFrame(data)

        # Find the model with the best score for the metric
        best_model = df[df[metric_name] == df[metric_name].max()]['Model'].values[0]
        print(f"Best model for {metric_name}: {best_model} ({df[metric_name].max()}%)")

        fig = px.bar(df, x='Model', y=metric_name, title=plot_title)
        fig.show()

    def plot_accuracy(self):
        self.plot_metric('Accuracy', 'Model Accuracy Comparison')

    def plot_specificity(self):
        self.plot_metric('Specificity', 'Model Specificity Comparison')

    def plot_sensitivity(self):
        self.plot_metric('Sensitivity', 'Model Sensitivity Comparison')

    def plot_precision(self):
        self.plot_metric('Precision', 'Model Precision Comparison')
model_results = model_results

model_comparison = ModelComparison(model_results)
**<font size="4" color="blue">Accuracy</font>**
# Plot accuracy
model_comparison.plot_accuracy()
**<font size="4" color="blue">Specificity</font>**
model_comparison.plot_specificity()
**<font size="4" color="blue">Sensitivity</font>**
model_comparison.plot_sensitivity()
**<font size="4" color="blue">Precision</font>**
model_comparison.plot_precision()
**<font size="4" color="blue">Classification Report</font>**
# Create a dictionary with the classification report data
classification_reports = {
    'baseline_model': {
        'glioma_tumor': {'precision': 0.95, 'recall': 0.77, 'f1-score': 0.85},
        'meningioma_tumor': {'precision': 0.75, 'recall': 0.84, 'f1-score': 0.79},
        'normal': {'precision': 0.78, 'recall': 0.90, 'f1-score': 0.83},
        'pituitary_tumor': {'precision': 0.88, 'recall': 0.88, 'f1-score': 0.88},
    },
    'model_1': {
        'glioma_tumor': {'precision': 0.94, 'recall': 0.97, 'f1-score': 0.95},
        'meningioma_tumor': {'precision': 0.96, 'recall': 0.90, 'f1-score': 0.93},
        'normal': {'precision': 0.99, 'recall': 0.95, 'f1-score': 0.97},
        'pituitary_tumor': {'precision': 0.94, 'recall': 0.99, 'f1-score': 0.97},
    },
    'model_2': {
        'glioma_tumor': {'precision': 0.91, 'recall': 0.94, 'f1-score': 0.93},
        'meningioma_tumor': {'precision': 0.92, 'recall': 0.90, 'f1-score': 0.91},
        'normal': {'precision': 0.97, 'recall': 0.98, 'f1-score': 0.97},
        'pituitary_tumor': {'precision': 0.96, 'recall': 0.95, 'f1-score': 0.96},
    },
    'model_3': {
        'glioma_tumor': {'precision': 0.93, 'recall': 0.95, 'f1-score': 0.94},
        'meningioma_tumor': {'precision': 0.96, 'recall': 0.96, 'f1-score': 0.96},
        'normal': {'precision': 0.95, 'recall': 1.00, 'f1-score': 0.97},
        'pituitary_tumor': {'precision': 0.99, 'recall': 0.94, 'f1-score': 0.97},
    }
}

# Create a summary table
summary_table = pd.DataFrame()

for model, class_metrics in classification_reports.items():
    for class_name, metrics in class_metrics.items():
        summary_table.at[model, f'{class_name}_precision'] = metrics['precision']
        summary_table.at[model, f'{class_name}_recall'] = metrics['recall']
        summary_table.at[model, f'{class_name}_f1-score'] = metrics['f1-score']

# Display summary table
summary_table
# Define the models and their metrics
models = ['baseline_model', 'model_1', 'model_2', 'model_3']
classes = ['glioma_tumor', 'meningioma_tumor', 'normal', 'pituitary_tumor']
metrics = ['precision', 'recall', 'f1-score']

# Create a figure with subplots for each class
fig, axes = plt.subplots(nrows=len(classes), ncols=1, figsize=(10, 20))

for i, class_name in enumerate(classes):
    ax = axes[i]

    # Create an array to hold the metric values for each model
    model_metrics = []
    for model in models:
        model_metrics.append([classification_reports[model][class_name][metric] for metric in metrics])

    # Number of metrics
    num_metrics = len(metrics)

    # Compute angle for each metric
    angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()
    angles += angles[:1]

    # Plot the spider plot for the current class
    for j, model in enumerate(models):
        ax.fill(angles, model_metrics[j] + [model_metrics[j][0]], alpha=0.1, label=model)

    # Set labels for each metric
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics)

    # Set the title for the current subplot
    ax.set_title(f'Performance for {class_name}')

    # Add a legend to the plot
    ax.legend(loc='upper right')

# Adjust subplot layout
plt.tight_layout()

# Show the spider plots
plt.show()

# **Model Deployment**
<font size="4>The brain tumor classification model has been successfully deployed using FastAPI. The deployed model is accessible via a web interface.

**Key Highlights:**

* The model is designed for real-time brain tumor classification.
* It achieves an accuracy of 95.14% and demonstrates high specificity, sensitivity, and precision.
* Ethical concerns have been addressed, and the model is intended for research and educational purposes only.

**How to Access the Model:**

* Click on the following link to access the deployed model:
<a href="http://127.0.0.1:8000">Brain Tumor Classification Web App</a>

* Upload an image for brain tumor classification.

* The model will provide classification results, including class label, class name, and confidence.

**Note:** The FastAPI web app must be running locally to access the model via the provided link. Others will require the code and a local environment to run the app themselves.</font>
# **Summary of Analysis**

<font size="4">The analysis assessed various models for classifying MRI brain scans into four categories: normal, glioma, meningioma, and pituitary tumors. The primary objective was to develop a classifier that ensures precise identification of these brain tumor types while also minimizing false positives in medical diagnostics.

Four models were analyzed using key metrics, including accuracy, specificity, sensitivity, precision, and F1-score. The findings are as follows:

**Model 1** emerged as the top performer with an accuracy of 95.14%, exceptional specificity (97.77%), strong sensitivity (93.68%), and impressive precision (97.60%).

**Model 3** closely followed with an accuracy of 95.62%, balanced specificity (96.61%), robust sensitivity (96.13%), and high precision (96.67%).

**Model 2** achieved notable accuracy (93.68%), high specificity (96.05%), strong sensitivity (96.13%), and precision (95.88%).

The **Baseline Model** displayed high sensitivity (96.82%) but had lower specificity (83.73%) and precision (84.92%).

**Model 1** was chosen as the preferred model due to its slightly superior specificity, precision, and sensitivity, which are pivotal in minimizing false positives and ensuring reliable diagnostic results. The research aims to advance medical diagnostics with deep learning models and provides recommendations for further enhancements in this context.
# **Recommendations**

* **<font size="4">Enhanced Model Training:** Consider extending the training duration and exploring more complex architectures, provided there are adequate computational resources. Prolonged training has the potential to improve accuracy, especially when building models from scratch.</font>

* **<font size="4>Incorporation of Additional Tumor Types:** To broaden the scope of brain tumor diagnostics, include additional tumor types beyond glioma, meningioma, and pituitary tumors. This comprehensive approach will enhance the applicability of medical diagnostics.</font>

* **<font size="4">Benign vs. Malignant Classification:** Given the critical importance of distinguishing between benign and malignant tumors in medical diagnosis, future research should encompass this classification to guide more precise and suitable treatment decisions.</font>

* **<font size="4">Larger and Diverse Datasets:** Expanding the dataset with a more extensive and diverse collection of MRI scans can better represent the entire spectrum of brain tumor cases. A larger dataset contributes to improved model generalization and performance.</font>

* **<font size="4">Exploration of Domain-Specific Models:** While transfer learning has shown effectiveness, there is potential to explore the development of domain-specific models tailored for the specific task of brain tumor classification.</font>

* **<font size="4">Resource Investment:** Enhance model training capabilities by investing in advanced computational resources. More powerful GPUs or dedicated hardware can expedite training and lead to higher accuracy.</font>

<font size="4">In conclusion, this research serves as a cornerstone for advanced medical diagnostics using deep learning models. The findings and recommendations provide a roadmap for future studies in the field of medical image classification, ultimately advancing patient care and healthcare efficiency. The overarching goal is to develop models that prioritize patient safety by minimizing false diagnoses in medical diagnostics while maintaining high accuracy.</font>
# **References**

Dubail, T. (2023). Brain Tumors (256x256) [Data set]. Kaggle. https://www.kaggle.com/datasets/thomasdubail/brain-tumors-256x256
